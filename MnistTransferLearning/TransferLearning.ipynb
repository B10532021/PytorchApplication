{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需要的包，请保证torchvision已经在你的环境中安装好\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# 设置图像读取器的超参数\n",
    "image_size = 28  #图像的总尺寸28*28\n",
    "num_classes = 10  #标签的种类数\n",
    "num_epochs = 20  #训练的总循环周期\n",
    "batch_size = 64  #批处理的尺寸大小\n",
    "\n",
    "# 如果系统中存在着GPU，我们将用GPU来完成张量的计算\n",
    "use_cuda = torch.cuda.is_available() #定义一个布尔型变量，标志当前的GPU是否可用\n",
    "\n",
    "# 如果当前GPU可用，则将优先在GPU上进行张量计算\n",
    "dtype = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "itype = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "\n",
    "# 加载MINIST数据，如果没有下载过，就会在当前路径下新建/data子目录，并把文件存放其中\n",
    "# MNIST数据是属于torchvision包自带的数据，所以可以直接调用。\n",
    "# 在调用自己的数据的时候，我们可以用torchvision.datasets.ImageFolder或者torch.utils.data.TensorDataset来加载\n",
    "train_dataset = dsets.MNIST(root='./data',  #文件存放路径\n",
    "                            train=True,   #提取训练集\n",
    "                            transform=transforms.ToTensor(),  #将图像转化为Tensor\n",
    "                            download=True)\n",
    "\n",
    "# 加载测试数据集\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# 定义两个采样器，每一个采样器都随机地从原始的数据集中抽样数据。抽样数据采用permutation\n",
    "# 生成任意一个下标重排，从而利用下标来提取dataset中的数据\n",
    "sample_size = len(train_dataset)\n",
    "sampler1 = torch.utils.data.sampler.SubsetRandomSampler(\n",
    "    np.random.choice(range(len(train_dataset)), sample_size))\n",
    "sampler2 = torch.utils.data.sampler.SubsetRandomSampler(\n",
    "    np.random.choice(range(len(train_dataset)), sample_size))\n",
    "\n",
    "# 定义两个加载器，分别封装了前两个采样器，实现采样。\n",
    "train_loader1 = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           sampler = sampler1\n",
    "                                           )\n",
    "train_loader2 = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           sampler = sampler2\n",
    "                                           )\n",
    "\n",
    "# 对于校验数据和测试数据，我们进行类似的处理。\n",
    "val_size = 5000\n",
    "val_indices1 = range(val_size)\n",
    "val_indices2 = np.random.permutation(range(val_size))\n",
    "test_indices1 = range(val_size, len(test_dataset))\n",
    "test_indices2 = np.random.permutation(test_indices1)\n",
    "val_sampler1 = torch.utils.data.sampler.SubsetRandomSampler(val_indices1)\n",
    "val_sampler2 = torch.utils.data.sampler.SubsetRandomSampler(val_indices2)\n",
    "\n",
    "test_sampler1 = torch.utils.data.sampler.SubsetRandomSampler(test_indices1)\n",
    "test_sampler2 = torch.utils.data.sampler.SubsetRandomSampler(test_indices2)\n",
    "\n",
    "val_loader1 = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = False,\n",
    "                                        sampler = val_sampler1\n",
    "                                        )\n",
    "val_loader2 = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                        batch_size = batch_size,\n",
    "                                        shuffle = False,\n",
    "                                        sampler = val_sampler2\n",
    "                                        )\n",
    "test_loader1 = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False,\n",
    "                                         sampler = test_sampler1\n",
    "                                         )\n",
    "test_loader2 = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False,\n",
    "                                         sampler = test_sampler2\n",
    "                                         )\n",
    "\n",
    "# 为了比较不同数据量对迁移学习的影响，我们设定了一个加载数据的比例fraction\n",
    "# 即我们只加载原训练数据集的1/fraction来训练网络\n",
    "fraction = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        output = F.log_softmax(output, dim = 1)\n",
    "        return output\n",
    "    \n",
    "CNN_net = torch.load('minst_conv_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9966"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rightness(predictions, labels):\n",
    "    \"\"\"计算预测错误率的函数，其中predictions是模型给出的一组预测结果，batch_size行10列的矩阵，labels是数据之中的正确答案\"\"\"\n",
    "    pred = torch.max(predictions.data, 1)[1] # 对于任意一行（一个样本）的输出值的第1个维度，求最大，得到每一行的最大元素的下标\n",
    "    rights = pred.eq(labels.data.view_as(pred)).sum() #将下标与labels中包含的类别进行比较，并累计得到比较正确的数量\n",
    "    return rights, len(labels) #返回正确的数量和这一次一共比较了多少元素\n",
    "\n",
    "#在测试集上分批运行，并计算总的正确率\n",
    "CNN_net.eval() #标志模型当前为运行阶段\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "vals = []\n",
    "\n",
    "#对测试数据集进行循环\n",
    "for data, target in test_loader1:\n",
    "#     data, target = data.clone().detach().requires_grad_(False), target.clone().detach()\n",
    "    with torch.no_grad():\n",
    "        data = data.clone().detach()\n",
    "    target = target.clone().detach()\n",
    "    \n",
    "    output = CNN_net(data) #将特征数据喂入网络，得到分类的输出\n",
    "    val = rightness(output, target) #获得正确样本数以及总样本数\n",
    "    vals.append(val) #记录结果\n",
    "\n",
    "#计算准确率\n",
    "rights = (sum([tup[0] for tup in vals]), sum([tup[1] for tup in vals]))\n",
    "right_rate = 1.0 * rights[0].numpy() / rights[1]\n",
    "right_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transfer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transfer, self).__init__()\n",
    "        self.net1_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1,\n",
    "                out_channels = 16, \n",
    "                kernel_size = 5,\n",
    "                stride = 1,\n",
    "                padding = 2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "        )\n",
    "        self.net1_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 16,\n",
    "                out_channels = 32,\n",
    "                kernel_size = 5,\n",
    "                stride = 1,\n",
    "                padding = 2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "        \n",
    "        self.net2_conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 1,\n",
    "                out_channels = 16, \n",
    "                kernel_size = 5,\n",
    "                stride = 1,\n",
    "                padding = 2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "        )\n",
    "        self.net2_conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels = 16,\n",
    "                out_channels = 32,\n",
    "                kernel_size = 5,\n",
    "                stride = 1,\n",
    "                padding = 2,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(2 * 32 * 7 * 7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2 * num_classes)\n",
    "        self.fc3 = nn.Linear(2 * num_classes, num_classes)\n",
    "        self.fc4 = nn.Linear(num_classes, 1)\n",
    "    \n",
    "    def forward(self, x, y, training = True):\n",
    "        x = self.net1_conv1(x)\n",
    "        x = self.net1_conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        y = self.net2_conv1(y)\n",
    "        y = self.net2_conv2(y)\n",
    "        y = y.view(y.size(0), -1)\n",
    "        \n",
    "        z = torch.cat((x, y), 1)\n",
    "        z = self.fc1(z)\n",
    "        z = F.relu(z)\n",
    "        z = F.dropout(z, training = self.training)\n",
    "        z = self.fc2(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.fc3(z)\n",
    "        z = F.relu(z)\n",
    "        z = self.fc4(z)\n",
    "        return z\n",
    "    \n",
    "    def set_filter_values(self, net):\n",
    "        self.net1_conv1[0].weight.data = copy.deepcopy(net.conv1[0].weight.data)\n",
    "        self.net1_conv1[0].bias.data = copy.deepcopy(net.conv1[0].bias.data)\n",
    "        self.net1_conv2[0].weight.data = copy.deepcopy(net.conv2[0].weight.data)\n",
    "        self.net1_conv2[0].bias.data = copy.deepcopy(net.conv2[0].bias.data)\n",
    "        \n",
    "        self.net2_conv1[0].weight.data = copy.deepcopy(net.conv1[0].weight.data)\n",
    "        self.net2_conv1[0].bias.data = copy.deepcopy(net.conv1[0].bias.data)\n",
    "        self.net2_conv2[0].weight.data = copy.deepcopy(net.conv2[0].weight.data)\n",
    "        self.net2_conv2[0].bias.data = copy.deepcopy(net.conv2[0].bias.data)\n",
    "        \n",
    "        self.net1_conv1 = self.net1_conv1.cuda() if use_cuda else self.net1_conv1\n",
    "        self.net1_conv2 = self.net1_conv2.cuda() if use_cuda else self.net1_conv2\n",
    "        \n",
    "        self.net2_conv1 = self.net2_conv1.cuda() if use_cuda else self.net2_conv1\n",
    "        self.net2_conv2 = self.net2_conv2.cuda() if use_cuda else self.net2_conv2\n",
    "    \n",
    "    def set_filter_values_notgrad(self, net):\n",
    "        set_filter_values()\n",
    "        \n",
    "        self.net1_conv1.weight.requires_grad = False\n",
    "        self.net1_conv1.bais.requires_grad = False\n",
    "        self.net1_conv2.weight.requires_grad = False\n",
    "        self.net1_conv2.bias.requires_grad = False\n",
    "        \n",
    "        self.net2_conv1.weight.requires_grad = False\n",
    "        self.net2_conv1.bais.requires_grad = False\n",
    "        self.net2_conv2.weight.requires_grad = False\n",
    "        self.net2_conv2.bias.requires_grad = False\n",
    "        \n",
    "\n",
    "def rightness(y, target):\n",
    "    out = torch.round(y.squeeze()).type(itype)\n",
    "    out = out.eq(target).sum()\n",
    "    out1 = y.size()[0]\n",
    "    return(out, out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start transfer -> pretrained transfer\n",
    "net = Transfer()\n",
    "net.set_filter_values(CNN_net)\n",
    "\n",
    "if use_cuda:\n",
    "    net = net.cuda()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "new_parameters = []\n",
    "for para in net.parameters():\n",
    "    if para.requires_grad:\n",
    "        new_parameters.append(para)\n",
    "\n",
    "optimizer = optim.Adam(new_parameters, lr = 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss：20.24, validation：16.74, accuracy：0.10\n",
      "epoch:0, train_loss：16.92, validation：16.88, accuracy：0.09\n",
      "epoch:0, train_loss：17.09, validation：16.63, accuracy：0.10\n",
      "epoch:0, train_loss：16.96, validation：16.73, accuracy：0.10\n",
      "epoch:0, train_loss：16.84, validation：17.09, accuracy：0.10\n",
      "epoch:0, train_loss：16.76, validation：17.31, accuracy：0.10\n",
      "epoch:0, train_loss：16.78, validation：16.78, accuracy：0.09\n",
      "epoch:0, train_loss：16.77, validation：16.95, accuracy：0.10\n",
      "epoch:0, train_loss：16.75, validation：16.85, accuracy：0.10\n",
      "epoch:0, train_loss：16.80, validation：16.97, accuracy：0.10\n",
      "epoch:1, train_loss：18.13, validation：16.27, accuracy：0.09\n",
      "epoch:1, train_loss：16.79, validation：16.88, accuracy：0.10\n",
      "epoch:1, train_loss：16.65, validation：16.58, accuracy：0.10\n",
      "epoch:1, train_loss：16.59, validation：16.83, accuracy：0.10\n",
      "epoch:1, train_loss：16.56, validation：16.96, accuracy：0.11\n",
      "epoch:1, train_loss：16.58, validation：17.07, accuracy：0.10\n",
      "epoch:1, train_loss：16.63, validation：16.76, accuracy：0.10\n",
      "epoch:1, train_loss：16.66, validation：16.38, accuracy：0.10\n",
      "epoch:1, train_loss：16.68, validation：16.27, accuracy：0.11\n",
      "epoch:1, train_loss：16.68, validation：16.61, accuracy：0.10\n",
      "epoch:2, train_loss：15.80, validation：16.83, accuracy：0.10\n",
      "epoch:2, train_loss：16.85, validation：16.59, accuracy：0.10\n",
      "epoch:2, train_loss：16.95, validation：16.76, accuracy：0.10\n",
      "epoch:2, train_loss：16.74, validation：16.59, accuracy：0.10\n",
      "epoch:2, train_loss：16.84, validation：16.91, accuracy：0.10\n",
      "epoch:2, train_loss：16.81, validation：16.47, accuracy：0.10\n",
      "epoch:2, train_loss：16.88, validation：16.60, accuracy：0.10\n",
      "epoch:2, train_loss：16.91, validation：16.32, accuracy：0.10\n",
      "epoch:2, train_loss：16.93, validation：16.72, accuracy：0.10\n",
      "epoch:2, train_loss：16.91, validation：16.60, accuracy：0.11\n",
      "epoch:3, train_loss：16.53, validation：16.39, accuracy：0.10\n",
      "epoch:3, train_loss：16.65, validation：16.57, accuracy：0.10\n",
      "epoch:3, train_loss：16.71, validation：16.44, accuracy：0.10\n",
      "epoch:3, train_loss：16.60, validation：16.61, accuracy：0.10\n",
      "epoch:3, train_loss：16.74, validation：17.14, accuracy：0.09\n",
      "epoch:3, train_loss：16.80, validation：16.65, accuracy：0.10\n",
      "epoch:3, train_loss：16.80, validation：16.42, accuracy：0.10\n",
      "epoch:3, train_loss：16.86, validation：16.51, accuracy：0.10\n",
      "epoch:3, train_loss：16.82, validation：16.92, accuracy：0.10\n",
      "epoch:3, train_loss：16.87, validation：17.57, accuracy：0.09\n",
      "epoch:4, train_loss：13.16, validation：16.70, accuracy：0.10\n",
      "epoch:4, train_loss：16.83, validation：16.88, accuracy：0.10\n",
      "epoch:4, train_loss：16.83, validation：16.95, accuracy：0.10\n",
      "epoch:4, train_loss：16.98, validation：16.69, accuracy：0.10\n",
      "epoch:4, train_loss：16.98, validation：16.82, accuracy：0.10\n",
      "epoch:4, train_loss：16.92, validation：17.14, accuracy：0.09\n",
      "epoch:4, train_loss：16.89, validation：16.85, accuracy：0.10\n",
      "epoch:4, train_loss：16.89, validation：16.82, accuracy：0.09\n",
      "epoch:4, train_loss：16.92, validation：16.65, accuracy：0.10\n",
      "epoch:4, train_loss：16.90, validation：16.65, accuracy：0.10\n",
      "epoch:5, train_loss：16.12, validation：16.69, accuracy：0.10\n",
      "epoch:5, train_loss：17.37, validation：16.59, accuracy：0.10\n",
      "epoch:5, train_loss：17.15, validation：16.84, accuracy：0.10\n",
      "epoch:5, train_loss：16.92, validation：17.21, accuracy：0.10\n",
      "epoch:5, train_loss：16.88, validation：16.85, accuracy：0.11\n",
      "epoch:5, train_loss：16.91, validation：17.52, accuracy：0.10\n",
      "epoch:5, train_loss：16.89, validation：16.40, accuracy：0.10\n",
      "epoch:5, train_loss：16.93, validation：16.85, accuracy：0.10\n",
      "epoch:5, train_loss：16.91, validation：16.95, accuracy：0.09\n",
      "epoch:5, train_loss：16.87, validation：16.78, accuracy：0.09\n",
      "epoch:6, train_loss：15.42, validation：16.16, accuracy：0.10\n",
      "epoch:6, train_loss：16.78, validation：16.38, accuracy：0.11\n",
      "epoch:6, train_loss：17.02, validation：16.96, accuracy：0.11\n",
      "epoch:6, train_loss：16.88, validation：16.30, accuracy：0.11\n",
      "epoch:6, train_loss：16.79, validation：16.74, accuracy：0.09\n",
      "epoch:6, train_loss：16.81, validation：16.58, accuracy：0.10\n",
      "epoch:6, train_loss：16.79, validation：16.56, accuracy：0.10\n",
      "epoch:6, train_loss：16.80, validation：17.08, accuracy：0.10\n",
      "epoch:6, train_loss：16.84, validation：17.00, accuracy：0.10\n",
      "epoch:6, train_loss：16.87, validation：16.69, accuracy：0.10\n",
      "epoch:7, train_loss：14.77, validation：17.05, accuracy：0.09\n",
      "epoch:7, train_loss：16.35, validation：16.90, accuracy：0.10\n",
      "epoch:7, train_loss：16.44, validation：16.97, accuracy：0.10\n",
      "epoch:7, train_loss：16.58, validation：16.61, accuracy：0.09\n",
      "epoch:7, train_loss：16.68, validation：16.62, accuracy：0.10\n",
      "epoch:7, train_loss：16.72, validation：16.52, accuracy：0.10\n",
      "epoch:7, train_loss：16.70, validation：16.96, accuracy：0.10\n",
      "epoch:7, train_loss：16.75, validation：17.24, accuracy：0.09\n",
      "epoch:7, train_loss：16.76, validation：16.68, accuracy：0.10\n",
      "epoch:7, train_loss：16.75, validation：16.79, accuracy：0.10\n",
      "epoch:8, train_loss：19.54, validation：17.08, accuracy：0.10\n",
      "epoch:8, train_loss：16.66, validation：16.88, accuracy：0.10\n",
      "epoch:8, train_loss：16.73, validation：16.90, accuracy：0.11\n",
      "epoch:8, train_loss：16.68, validation：16.71, accuracy：0.10\n",
      "epoch:8, train_loss：16.70, validation：16.67, accuracy：0.10\n",
      "epoch:8, train_loss：16.71, validation：16.64, accuracy：0.11\n",
      "epoch:8, train_loss：16.70, validation：16.97, accuracy：0.10\n",
      "epoch:8, train_loss：16.73, validation：16.61, accuracy：0.10\n",
      "epoch:8, train_loss：16.72, validation：17.12, accuracy：0.10\n",
      "epoch:8, train_loss：16.75, validation：16.73, accuracy：0.10\n",
      "epoch:9, train_loss：14.40, validation：17.00, accuracy：0.09\n",
      "epoch:9, train_loss：16.90, validation：17.14, accuracy：0.10\n",
      "epoch:9, train_loss：16.93, validation：16.54, accuracy：0.10\n",
      "epoch:9, train_loss：16.84, validation：16.58, accuracy：0.10\n",
      "epoch:9, train_loss：16.75, validation：16.75, accuracy：0.11\n",
      "epoch:9, train_loss：16.81, validation：17.02, accuracy：0.10\n",
      "epoch:9, train_loss：16.87, validation：17.00, accuracy：0.10\n",
      "epoch:9, train_loss：16.85, validation：16.73, accuracy：0.10\n",
      "epoch:9, train_loss：16.88, validation：16.94, accuracy：0.10\n",
      "epoch:9, train_loss：16.90, validation：16.75, accuracy：0.10\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "records = []\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for step, data in enumerate(zip(train_loader1, train_loader2)):\n",
    "        if step >= (len(train_loader1) // fraction):\n",
    "            break\n",
    "        \n",
    "        ((x1, y1), (x2, y2)) = data\n",
    "        if use_cuda:\n",
    "            x1, y1, x2, y2 = x1.cuda(), y1.cuda(), x2.cuda(), y2.cuda()\n",
    "        \n",
    "        net.train()\n",
    "        outputs = net(Variable(x1), Variable(x2))\n",
    "        labels = y1 + y2\n",
    "        loss = criterion(outputs, labels.type(torch.float))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        losses.append(loss.data.numpy())\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            val_losses = []\n",
    "            rights = []\n",
    "            net.eval()\n",
    "            for val_data in zip(val_loader1, val_loader2):\n",
    "                ((x1, y1), (x2, y2)) = val_data\n",
    "                if use_cuda:\n",
    "                    x1, y1, x2, y2 = x1.cuda(), y1.cuda(), x2.cuda(), y2.cuda()\n",
    "                outputs = net(Variable(x1), Variable(x2))\n",
    "                labels = y1 + y2\n",
    "                loss = criterion(outputs, labels.type(torch.float))\n",
    "                loss = loss.cpu() if use_cuda else loss\n",
    "                val_losses.append(loss.data.numpy())\n",
    "                \n",
    "                right = rightness(outputs.data, labels)\n",
    "                rights.append(right)\n",
    "            right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "            print('epoch:{}, train_loss：{:.2f}, validation：{:.2f}, accuracy：{:.2f}'.format(\n",
    "                epoch, np.mean(losses), np.mean(val_losses), right_ratio))\n",
    "            records.append([np.mean(losses), np.mean(val_losses), right_ratio])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
